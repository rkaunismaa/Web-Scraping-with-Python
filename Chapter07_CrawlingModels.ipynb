{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saturday, November 23, 2024\n",
    "\n",
    "mamba activate ws\n",
    "\n",
    "Very little of this code works! I'm not sure why.  I'm going to try to get it working on my own ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with different website layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "    \n",
    "    def print(self):\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'BODY:\\n {self.body}')\n",
    "\n",
    "def scrapeCNN(url):\n",
    "    bs = BeautifulSoup(urlopen(url))\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class': 'article__content'}).text\n",
    "    print('body: ')\n",
    "    print(body)\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = BeautifulSoup(urlopen(url))\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class': 'post-body'}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "# url = 'https://www.brookings.edu/research/robotic-rulemaking/'\n",
    "# content = scrapeBrookings(url)\n",
    "# content.print()\n",
    "\n",
    "# url = 'https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html'\n",
    "# content = scrapeCNN(url)\n",
    "# content.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.brookings.edu/research/robotic-rulemaking/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mscrapeBrookings\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m content\u001b[38;5;241m.\u001b[39mprint()\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mscrapeBrookings\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrapeBrookings\u001b[39m(url):\n\u001b[0;32m---> 24\u001b[0m     bs \u001b[38;5;241m=\u001b[39m BeautifulSoup(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m     title \u001b[38;5;241m=\u001b[39m bs\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     26\u001b[0m     body \u001b[38;5;241m=\u001b[39m bs\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost-body\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/miniforge3/envs/ws/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ws/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/ws/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/ws/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ws/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/ws/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "url = 'https://www.brookings.edu/research/robotic-rulemaking/'\n",
    "content = scrapeBrookings(url)\n",
    "content.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body: \n",
      "\n",
      "\n",
      "\n",
      "New York\n",
      "CNN\n",
      "         — \n",
      "    \n",
      "\n",
      "\n",
      "            Twitter’s traditional bird icon was booted and replaced with an image of a Shiba Inu, an apparent nod to dogecoin, the joke cryptocurrency that CEO Elon Musk is being sued over. \n",
      "    \n",
      "\n",
      "            Musk addressed the change Monday afternoon, tweeting, “as promised” above an image of a year-old conversation in which another user suggested that Musk “just buy Twitter” and “change the bird logo to a doge.” \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CNN/Adobe Stock\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Elon Musk's Twitter promised a purge of blue check marks. Instead he singled out one account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            The doge logo appeared on the site two days after Musk asked a judge to throw out a $258 billion racketeering lawsuit accusing him of running a pyramid scheme to support the dogecoin, according to Reuters.\n",
      "\n",
      "\n",
      "            Lawyers for Musk and Tesla called the lawsuit by dogecoin investors a “fanciful work of fiction” over Musk’s “innocuous and often silly tweets.”\n",
      "    \n",
      "\n",
      "            It wasn’t clear whether the logo change was permanent. Musk has been known to use Twitter to troll both his fans and his critics. \n",
      "    \n",
      "\n",
      "            The price of dogecoin, which is typically volatile, was up more than 20% over the past 24 hours, to about 9 cents. It was trading just under 8 cents Monday morning.\n",
      "    \n",
      "\n",
      "Dogecoin was created December 6, 2013, by a pair of software engineers — as a joke. The name is a nod to the “doge” meme that became popular a decade ago. Its Shiba Inu mascot mimicks that meme: a dog surrounded by a bunch of Comic Sans text in broken English.\n",
      "    \n",
      "\n",
      "TITLE: \n",
      "      Dogecoin jumps after Elon Musk replaces Twitter bird with Shiba Inu\n",
      "    \n",
      "URL: https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html\n",
      "BODY:\n",
      " \n",
      "\n",
      "\n",
      "New York\n",
      "CNN\n",
      "         — \n",
      "    \n",
      "\n",
      "\n",
      "            Twitter’s traditional bird icon was booted and replaced with an image of a Shiba Inu, an apparent nod to dogecoin, the joke cryptocurrency that CEO Elon Musk is being sued over. \n",
      "    \n",
      "\n",
      "            Musk addressed the change Monday afternoon, tweeting, “as promised” above an image of a year-old conversation in which another user suggested that Musk “just buy Twitter” and “change the bird logo to a doge.” \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CNN/Adobe Stock\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Elon Musk's Twitter promised a purge of blue check marks. Instead he singled out one account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            The doge logo appeared on the site two days after Musk asked a judge to throw out a $258 billion racketeering lawsuit accusing him of running a pyramid scheme to support the dogecoin, according to Reuters.\n",
      "\n",
      "\n",
      "            Lawyers for Musk and Tesla called the lawsuit by dogecoin investors a “fanciful work of fiction” over Musk’s “innocuous and often silly tweets.”\n",
      "    \n",
      "\n",
      "            It wasn’t clear whether the logo change was permanent. Musk has been known to use Twitter to troll both his fans and his critics. \n",
      "    \n",
      "\n",
      "            The price of dogecoin, which is typically volatile, was up more than 20% over the past 24 hours, to about 9 cents. It was trading just under 8 cents Monday morning.\n",
      "    \n",
      "\n",
      "Dogecoin was created December 6, 2013, by a pair of software engineers — as a joke. The name is a nod to the “doge” meme that became popular a decade ago. Its Shiba Inu mascot mimicks that meme: a dog surrounded by a bunch of Comic Sans text in broken English.\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html'\n",
    "content = scrapeCNN(url)\n",
    "content.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    Common base class for all articles/pages\n",
    "    \"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'BODY:\\n{self.body}')\n",
    "\n",
    "class Website:\n",
    "    \"\"\" \n",
    "    Contains information about website structure\n",
    "    \"\"\"\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def getPage(url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except Exception:\n",
    "            return None\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def safeGet(bs, selector):\n",
    "        \"\"\"\n",
    "        Utilty function used to get a content string from a Beautiful Soup\n",
    "        object and a selector. Returns an empty string if no object\n",
    "        is found for the given selector\n",
    "        \"\"\"\n",
    "        selectedElems = bs.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def getContent(website, path):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        url = website.url+path\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs, website.titleTag)\n",
    "            body = Crawler.safeGet(bs, website.bodyTag)\n",
    "            return Content(url, title, body)\n",
    "        return Content(url, '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'https://www.oreilly.com', 'h1', 'div.title-description'],\n",
    "    ['Reuters', 'https://www.reuters.com', 'h1', 'div.ArticleBodyWrapper'],\n",
    "    ['Brookings', 'https://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "    ['CNN', 'https://www.cnn.com', 'h1', 'div.article__content']\n",
    "]\n",
    "websites = []\n",
    "for name, url, title, body in siteData:\n",
    "    websites.append(Website(name, url, title, body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.oreilly.com/library/view/web-scraping-with/9781491910283\n",
      "TITLE: Web Scraping with Python\n",
      "BODY:\n",
      "\n",
      "\n",
      "\n",
      "      \n",
      "        Book\n",
      "      description\n",
      "Learn web scraping and crawling techniques to access unlimited data from any web source in any format. With this practical guide, you’ll learn how to use Python scripts and web APIs to gather and process data from thousands—or even millions—of web pages at once.Ideal for programmers, security professionals, and web administrators familiar with Python, this book not only teaches basic web scraping mechanics, but also delves into more advanced topics, such as analyzing raw data or using scrapers for frontend website testing. Code samples are available to help you understand the concepts in practice.\n",
      "Show and hide more\n",
      "\n",
      "Publisher resources\n",
      "View/Submit Errata\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Crawler.getContent(websites[0], '/library/view/web-scraping-with/9781491910283').print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0\n",
      "TITLE: \n",
      "BODY:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Crawler.getContent(websites[1], '/article/us-usa-epa-pruitt-idUSKBN19W2D0').print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/\n",
      "TITLE: \n",
      "BODY:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Crawler.getContent(websites[2],'/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/').print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html\n",
      "TITLE: \n",
      "      Dogecoin jumps after Elon Musk replaces Twitter bird with Shiba Inu\n",
      "    \n",
      "BODY:\n",
      "\n",
      "\n",
      "\n",
      "New York\n",
      "CNN\n",
      "         — \n",
      "    \n",
      "\n",
      "\n",
      "            Twitter’s traditional bird icon was booted and replaced with an image of a Shiba Inu, an apparent nod to dogecoin, the joke cryptocurrency that CEO Elon Musk is being sued over. \n",
      "    \n",
      "\n",
      "            Musk addressed the change Monday afternoon, tweeting, “as promised” above an image of a year-old conversation in which another user suggested that Musk “just buy Twitter” and “change the bird logo to a doge.” \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CNN/Adobe Stock\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Elon Musk's Twitter promised a purge of blue check marks. Instead he singled out one account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            The doge logo appeared on the site two days after Musk asked a judge to throw out a $258 billion racketeering lawsuit accusing him of running a pyramid scheme to support the dogecoin, according to Reuters.\n",
      "\n",
      "\n",
      "            Lawyers for Musk and Tesla called the lawsuit by dogecoin investors a “fanciful work of fiction” over Musk’s “innocuous and often silly tweets.”\n",
      "    \n",
      "\n",
      "            It wasn’t clear whether the logo change was permanent. Musk has been known to use Twitter to troll both his fans and his critics. \n",
      "    \n",
      "\n",
      "            The price of dogecoin, which is typically volatile, was up more than 20% over the past 24 hours, to about 9 cents. It was trading just under 8 cents Monday morning.\n",
      "    \n",
      "\n",
      "Dogecoin was created December 6, 2013, by a pair of software engineers — as a joke. The name is a nod to the “doge” meme that became popular a decade ago. Its Shiba Inu mascot mimicks that meme: a dog surrounded by a bunch of Comic Sans text in broken English.\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Crawler.getContent(websites[3], '/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html').print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling through sites with search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print(f'New article found for topic: {self.topic}')\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'BODY:\\n{self.body}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Contains information about website structure\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, website):\n",
    "        self.site = website\n",
    "        self.found = {}\n",
    "\n",
    "    def getPage(url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def safeGet(bs, selector):\n",
    "        \"\"\"\n",
    "        Utilty function used to get a content string from a Beautiful Soup\n",
    "        object and a selector. Returns an empty string if no object\n",
    "        is found for the given selector\n",
    "        \"\"\"\n",
    "        selectedElems = bs.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def getContent(self, topic, url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs, self.site.titleTag)\n",
    "            body = Crawler.safeGet(bs, self.site.bodyTag)\n",
    "            return Content(topic, url, title, body)\n",
    "        return Content(topic, url, '', '')\n",
    "\n",
    "    def search(self, topic):\n",
    "        \"\"\"\n",
    "        Searches a given website for a given topic and records all pages found\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(self.site.searchUrl + topic)\n",
    "        searchResults = bs.select(self.site.resultListing)\n",
    "        for result in searchResults:\n",
    "            url = result.select(self.site.resultUrl)[0].attrs['href']\n",
    "            # Check to see whether it's a relative or an absolute URL\n",
    "            url = url if self.site.absoluteUrl else self.site.url + url\n",
    "            if url not in self.found:\n",
    "                self.found[url] = self.getContent(topic, url)\n",
    "            self.found[url].print()\n",
    "\n",
    "\n",
    "\n",
    "# siteData = [\n",
    "#     ['Reuters', 'http://reuters.com', 'https://www.reuters.com/search/news?blob=', 'div.search-result-indiv',\n",
    "#         'h3.search-result-title a', False, 'h1', 'div.ArticleBodyWrapper'],\n",
    "#     ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
    "#         'div.article-info', 'h4.title a', True, 'h1', 'div.core-block']\n",
    "# ]\n",
    "# sites = []\n",
    "# for name, url, search, rListing, rUrl, absUrl, tt, bt in siteData:\n",
    "#     sites.append(Website(name, url, search, rListing, rUrl, absUrl, tt, bt))\n",
    "\n",
    "# crawlers = [Crawler(site) for site in sites]\n",
    "# topics = ['python', 'data%20science']\n",
    "\n",
    "# for topic in topics:\n",
    "#     for crawler in crawlers:\n",
    "#         crawler.search(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siteData = [\n",
    "    \n",
    "    ['Reuters', 'http://reuters.com', 'https://www.reuters.com/search/news?blob=', 'div.search-result-indiv',\n",
    "        'h3.search-result-title a', False, 'h1', 'div.ArticleBodyWrapper'],\n",
    "\n",
    "    ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
    "        'div.article-info', 'h4.title a', True, 'h1', 'div.core-block']\n",
    "        \n",
    "]\n",
    "\n",
    "# siteData = [\n",
    "#     ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
    "#         'div.article-info', 'h4.title a', True, 'h1', 'div.core-block']\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = []\n",
    "for name, url, search, rListing, rUrl, absUrl, tt, bt in siteData:\n",
    "    sites.append(Website(name, url, search, rListing, rUrl, absUrl, tt, bt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawlers = [Crawler(site) for site in sites]\n",
    "topics = ['python', 'data%20science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topics:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m crawler \u001b[38;5;129;01min\u001b[39;00m crawlers:\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mcrawler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 42\u001b[0m, in \u001b[0;36mCrawler.search\u001b[0;34m(self, topic)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mSearches a given website for a given topic and records all pages found\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m bs \u001b[38;5;241m=\u001b[39m Crawler\u001b[38;5;241m.\u001b[39mgetPage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite\u001b[38;5;241m.\u001b[39msearchUrl \u001b[38;5;241m+\u001b[39m topic)\n\u001b[0;32m---> 42\u001b[0m searchResults \u001b[38;5;241m=\u001b[39m \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite\u001b[38;5;241m.\u001b[39mresultListing)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m searchResults:\n\u001b[1;32m     44\u001b[0m     url \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite\u001b[38;5;241m.\u001b[39mresultUrl)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'select'"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    for crawler in crawlers:\n",
    "        crawler.search(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Sites through Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "\n",
    "    def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.targetPattern = targetPattern\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "\n",
    "class Content:\n",
    "\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'BODY:\\n{self.body}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "        self.visited = {}\n",
    "\n",
    "    def getPage(url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def safeGet(bs, selector):\n",
    "        selectedElems = bs.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def getContent(self, url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs, self.site.titleTag)\n",
    "            body = Crawler.safeGet(bs, self.site.bodyTag)\n",
    "            return Content(url, title, body)\n",
    "        return Content(url, '', '')\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Get pages from website home page\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(self.site.url)\n",
    "        targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern))\n",
    "        for targetPage in targetPages:\n",
    "            url = targetPage.attrs['href']\n",
    "            url = url if self.site.absoluteUrl else f'{self.site.url}{targetPage}'\n",
    "            if url not in self.visited:\n",
    "                self.visited[url] = self.getContent(url)\n",
    "                self.visited[url].print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 403: Forbidden\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m brookings \u001b[38;5;241m=\u001b[39m Website(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReuters\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://brookings.edu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/(research|blog)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv.post-body\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m crawler \u001b[38;5;241m=\u001b[39m Crawler(brookings)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcrawler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 39\u001b[0m, in \u001b[0;36mCrawler.crawl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03mGet pages from website home page\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m bs \u001b[38;5;241m=\u001b[39m Crawler\u001b[38;5;241m.\u001b[39mgetPage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m---> 39\u001b[0m targetPages \u001b[38;5;241m=\u001b[39m \u001b[43mbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindAll\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, href\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msite\u001b[38;5;241m.\u001b[39mtargetPattern))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m targetPage \u001b[38;5;129;01min\u001b[39;00m targetPages:\n\u001b[1;32m     41\u001b[0m     url \u001b[38;5;241m=\u001b[39m targetPage\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "brookings = Website('Reuters', 'https://brookings.edu', '\\/(research|blog)\\/', True, 'h1', 'div.post-body')\n",
    "crawler = Crawler(brookings)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling multiple page types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product(Website):\n",
    "    \"\"\"Contains information for scraping a product page\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, productNumber, price):\n",
    "        Website.__init__(self, name, url, TitleTag)\n",
    "        self.productNumberTag = productNumberTag\n",
    "        self.priceTag = priceTag\n",
    "\n",
    "class Article(Website):\n",
    "    \"\"\"Contains information for scraping an article page\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag, dateTag):\n",
    "        Website.__init__(self, name, url, titleTag)\n",
    "        self.bodyTag = bodyTag\n",
    "        self.dateTag = dateTag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
